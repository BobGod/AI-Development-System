"""
ğŸ¤– AIæ™ºèƒ½æ–°é—»èšåˆå¹³å° - æ–°é—»æ”¶é›†çˆ¬è™«å¼•æ“
è¶…è¶Šé‡å­ä½ã€æœºå™¨ä¹‹å¿ƒçš„å…¨è‡ªåŠ¨åŒ–æ–°é—»æ”¶é›†ç³»ç»Ÿ
"""

import asyncio
import aiohttp
import feedparser
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from urllib.parse import urljoin, urlparse
import hashlib
import re
from bs4 import BeautifulSoup
import requests
from concurrent.futures import ThreadPoolExecutor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    title: str
    content: str
    url: str
    source: str
    published_time: datetime
    author: str = ""
    tags: List[str] = None
    heat_score: float = 0.0
    content_type: str = "article"  # article, paper, tool, product
    language: str = "zh"
    summary: str = ""
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []
        # ç”Ÿæˆå”¯ä¸€ID
        self.id = hashlib.md5(f"{self.url}{self.title}".encode()).hexdigest()[:16]

class NewsSpider:
    """é«˜æ€§èƒ½æ–°é—»çˆ¬è™«å¼•æ“"""
    
    def __init__(self):
        self.session = None
        self.collected_urls = set()  # é˜²é‡å¤
        self.ai_keywords = [
            "äººå·¥æ™ºèƒ½", "AI", "GPT", "ChatGPT", "Claude", "å¤§æ¨¡å‹", "LLM",
            "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "è‡ªç„¶è¯­è¨€å¤„ç†", "NLP",
            "è®¡ç®—æœºè§†è§‰", "OpenAI", "Google AI", "ç™¾åº¦AI", "è…¾è®¯AI",
            "è‡ªåŠ¨é©¾é©¶", "æœºå™¨äºº", "æ™ºèƒ½åŠ©æ‰‹", "AIGC", "Stable Diffusion",
            "Midjourney", "æ–‡å¿ƒä¸€è¨€", "é€šä¹‰åƒé—®", "æ˜Ÿç«", "æ··å…ƒ"
        ]
        
        # å¤šè¯­è¨€æ•°æ®æºé…ç½®
        self.data_sources = {
            "chinese_media": [
                {
                    "name": "é‡å­ä½",
                    "rss": "https://www.qbitai.com/feed",
                    "website": "https://www.qbitai.com",
                    "encoding": "utf-8",
                    "language": "zh"
                },
                {
                    "name": "æœºå™¨ä¹‹å¿ƒ", 
                    "rss": "https://www.jiqizhixin.com/rss",
                    "website": "https://www.jiqizhixin.com",
                    "encoding": "utf-8",
                    "language": "zh"
                },
                {
                    "name": "é›·é”‹ç½‘AI",
                    "rss": "https://www.leiphone.com/category/ai/feed",
                    "website": "https://www.leiphone.com",
                    "encoding": "utf-8", 
                    "language": "zh"
                },
                {
                    "name": "36æ°ªAI",
                    "rss": "https://36kr.com/motif/327686782977.rss",
                    "website": "https://36kr.com",
                    "encoding": "utf-8",
                    "language": "zh"
                },
                {
                    "name": "æ–°æ™ºå…ƒ",
                    "website": "https://www.ai-techreview.com",
                    "selectors": {
                        "article_list": ".article-item",
                        "title": ".title",
                        "url": "a",
                        "time": ".time"
                    },
                    "language": "zh"
                }
            ],
            "international_media": [
                {
                    "name": "TechCrunch AI",
                    "rss": "https://techcrunch.com/category/artificial-intelligence/feed/",
                    "website": "https://techcrunch.com",
                    "language": "en"
                },
                {
                    "name": "VentureBeat AI",
                    "rss": "https://venturebeat.com/ai/feed/",
                    "website": "https://venturebeat.com",
                    "language": "en"
                },
                {
                    "name": "The Verge AI",
                    "rss": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                    "website": "https://www.theverge.com",
                    "language": "en"
                }
            ],
            "research_sources": [
                {
                    "name": "arXiv AI",
                    "rss": "https://rss.arxiv.org/rss/cs.AI",
                    "website": "https://arxiv.org",
                    "content_type": "paper",
                    "language": "en"
                },
                {
                    "name": "HuggingFace Blog",
                    "rss": "https://huggingface.co/blog/feed.xml",
                    "website": "https://huggingface.co",
                    "language": "en"
                },
                {
                    "name": "OpenAI Blog",
                    "rss": "https://openai.com/blog/rss.xml",
                    "website": "https://openai.com",
                    "language": "en"
                }
            ]
        }

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Mozilla/5.0 (compatible; AI-News-Aggregator/1.0; +https://ai-news-aggregator.com/bot)'
            }
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()

    def is_ai_related(self, title: str, content: str = "") -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAIç›¸å…³å†…å®¹"""
        text = (title + " " + content).lower()
        return any(keyword.lower() in text for keyword in self.ai_keywords)

    def calculate_heat_score(self, item: NewsItem) -> float:
        """è®¡ç®—æ–°é—»çƒ­åº¦åˆ†æ•°"""
        score = 0.0
        
        # æ—¶é—´å› å­ (è¶Šæ–°è¶Šçƒ­)
        hours_ago = (datetime.now() - item.published_time).total_seconds() / 3600
        if hours_ago <= 1:
            score += 50
        elif hours_ago <= 6:
            score += 30
        elif hours_ago <= 24:
            score += 20
        elif hours_ago <= 72:
            score += 10
        
        # å…³é”®è¯çƒ­åº¦
        hot_keywords = {
            "GPT-5": 30, "Claude-4": 25, "GPT-4": 20, "ChatGPT": 15,
            "OpenAI": 15, "è°·æ­Œ": 10, "ç™¾åº¦": 8, "è…¾è®¯": 8,
            "å‘å¸ƒ": 15, "çªç ´": 20, "é¦–æ¬¡": 18, "è¶…è¶Š": 22,
            "èèµ„": 12, "æ”¶è´­": 15, "IPO": 20, "ä¼°å€¼": 10
        }
        
        title_lower = item.title.lower()
        for keyword, points in hot_keywords.items():
            if keyword.lower() in title_lower:
                score += points
        
        # å†…å®¹é•¿åº¦å’Œè´¨é‡
        if len(item.content) > 1000:
            score += 10
        elif len(item.content) > 500:
            score += 5
            
        # æ¥æºæƒå¨æ€§
        source_weights = {
            "é‡å­ä½": 20, "æœºå™¨ä¹‹å¿ƒ": 20, "OpenAI": 25, "Google": 20,
            "TechCrunch": 18, "The Verge": 15, "arXiv": 22
        }
        score += source_weights.get(item.source, 5)
        
        return min(score, 100)  # æœ€é«˜100åˆ†

    async def fetch_rss_feed(self, source: Dict) -> List[NewsItem]:
        """è·å–RSSè®¢é˜…æº"""
        news_items = []
        
        try:
            async with self.session.get(source["rss"]) as response:
                if response.status != 200:
                    logger.warning(f"RSSè·å–å¤±è´¥: {source['name']} - {response.status}")
                    return news_items
                
                content = await response.text()
                feed = feedparser.parse(content)
                
                logger.info(f"ğŸ“¡ è·å–RSS: {source['name']} - {len(feed.entries)}æ¡")
                
                for entry in feed.entries[:20]:  # é™åˆ¶æ¯ä¸ªæºæœ€å¤š20æ¡
                    try:
                        # è§£æå‘å¸ƒæ—¶é—´
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_time = datetime(*entry.published_parsed[:6])
                        else:
                            pub_time = datetime.now() - timedelta(hours=1)
                        
                        # åªå¤„ç†24å°æ—¶å†…çš„æ–°é—»
                        if (datetime.now() - pub_time).days > 1:
                            continue
                            
                        # æå–å†…å®¹
                        content = ""
                        if hasattr(entry, 'summary'):
                            content = BeautifulSoup(entry.summary, 'html.parser').get_text()
                        elif hasattr(entry, 'content'):
                            content = BeautifulSoup(entry.content[0].value, 'html.parser').get_text()
                        
                        # æ£€æŸ¥æ˜¯å¦AIç›¸å…³
                        if not self.is_ai_related(entry.title, content):
                            continue
                        
                        # é˜²é‡å¤
                        if entry.link in self.collected_urls:
                            continue
                        self.collected_urls.add(entry.link)
                        
                        news_item = NewsItem(
                            title=entry.title.strip(),
                            content=content[:2000],  # é™åˆ¶é•¿åº¦
                            url=entry.link,
                            source=source["name"],
                            published_time=pub_time,
                            author=getattr(entry, 'author', ''),
                            language=source.get("language", "zh"),
                            content_type=source.get("content_type", "article")
                        )
                        
                        # è®¡ç®—çƒ­åº¦
                        news_item.heat_score = self.calculate_heat_score(news_item)
                        
                        news_items.append(news_item)
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†RSSæ¡ç›®å¤±è´¥: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"è·å–RSSå¤±è´¥ {source['name']}: {e}")
        
        return news_items

    async def scrape_website(self, source: Dict) -> List[NewsItem]:
        """çˆ¬å–ç½‘ç«™å†…å®¹"""
        news_items = []
        
        try:
            async with self.session.get(source["website"]) as response:
                if response.status != 200:
                    return news_items
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # è¿™é‡Œéœ€è¦æ ¹æ®ä¸åŒç½‘ç«™çš„ç»“æ„æ¥è§£æ
                # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦ä¸ºæ¯ä¸ªç½‘ç«™å®šåˆ¶é€‰æ‹©å™¨
                articles = soup.find_all(['article', 'div'], class_=re.compile(r'(article|post|news|item)', re.I))
                
                for article in articles[:10]:  # é™åˆ¶æ•°é‡
                    try:
                        title_elem = article.find(['h1', 'h2', 'h3', 'h4', 'a'])
                        if not title_elem:
                            continue
                            
                        title = title_elem.get_text().strip()
                        if not title or not self.is_ai_related(title):
                            continue
                            
                        url = title_elem.get('href') or article.find('a')['href']
                        if url and not url.startswith('http'):
                            url = urljoin(source["website"], url)
                        
                        if url in self.collected_urls:
                            continue
                        self.collected_urls.add(url)
                        
                        # è·å–å†…å®¹æ‘˜è¦
                        content_elem = article.find(['p', 'div'])
                        content = content_elem.get_text()[:500] if content_elem else ""
                        
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=url,
                            source=source["name"],
                            published_time=datetime.now() - timedelta(minutes=30),
                            language=source.get("language", "zh")
                        )
                        
                        news_item.heat_score = self.calculate_heat_score(news_item)
                        news_items.append(news_item)
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†ç½‘ç«™æ¡ç›®å¤±è´¥: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"çˆ¬å–ç½‘ç«™å¤±è´¥ {source['name']}: {e}")
        
        return news_items

    async def collect_all_news(self) -> List[NewsItem]:
        """æ”¶é›†æ‰€æœ‰æ–°é—»æº"""
        all_news = []
        tasks = []
        
        # æ”¶é›†æ‰€æœ‰RSSä»»åŠ¡
        for category, sources in self.data_sources.items():
            for source in sources:
                if "rss" in source:
                    tasks.append(self.fetch_rss_feed(source))
                else:
                    tasks.append(self.scrape_website(source))
        
        logger.info(f"ğŸš€ å¼€å§‹æ”¶é›†æ–°é—»ï¼Œæ€»å…±{len(tasks)}ä¸ªæ•°æ®æº...")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ±‡æ€»ç»“æœ
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {result}")
            elif isinstance(result, list):
                all_news.extend(result)
        
        # å»é‡å’Œæ’åº
        seen_titles = set()
        unique_news = []
        
        for news in all_news:
            title_key = news.title.lower().strip()
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        # æŒ‰çƒ­åº¦æ’åº
        unique_news.sort(key=lambda x: x.heat_score, reverse=True)
        
        logger.info(f"âœ… æ”¶é›†å®Œæˆï¼å…±è·å¾—{len(unique_news)}æ¡AIç›¸å…³æ–°é—»")
        
        return unique_news[:50]  # è¿”å›çƒ­åº¦æœ€é«˜çš„50æ¡

    def save_to_json(self, news_items: List[NewsItem], filename: str = None):
        """ä¿å­˜åˆ°JSONæ–‡ä»¶"""
        if filename is None:
            filename = f"ai_news_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        
        data = {
            "collected_time": datetime.now().isoformat(),
            "total_count": len(news_items),
            "news_items": [asdict(item) for item in news_items]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“ æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        return filename

# å®æ—¶æ–°é—»æ”¶é›†ä»»åŠ¡
async def collect_realtime_news():
    """å®æ—¶æ–°é—»æ”¶é›†ä¸»å‡½æ•°"""
    async with NewsSpider() as spider:
        news_items = await spider.collect_all_news()
        
        # ä¿å­˜æ•°æ®
        filename = spider.save_to_json(news_items)
        
        # æ‰“å°çƒ­é—¨æ–°é—»
        print("\nğŸ”¥ ä»Šæ—¥AIçƒ­é—¨æ–°é—» Top 10:")
        print("=" * 80)
        
        for i, news in enumerate(news_items[:10], 1):
            print(f"{i:2d}. [{news.heat_score:4.1f}åˆ†] {news.title}")
            print(f"    ğŸ“ {news.source} | â° {news.published_time.strftime('%H:%M')} | ğŸŒ {news.language}")
            print(f"    ğŸ“ {news.content[:100]}...")
            print("-" * 80)
        
        return news_items

if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ
    asyncio.run(collect_realtime_news())